{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4390,
     "status": "ok",
     "timestamp": 1620634088736,
     "user": {
      "displayName": "Terne Thorn Jakobsen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giyf9v1H1h4iteKo0T2fl5hHuAr9jxrMjcMwTDW=s64",
      "userId": "17600962664087183096"
     },
     "user_tz": -120
    },
    "id": "D3Z4iTczgZBC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import tensorflow.keras as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dKOAMn2gZBJ"
   },
   "source": [
    "# DEEP LEARNING 1 Exercises\n",
    "Today we will practice the basics. First some Linear Algebra using numpy. Remember that working with vectors and matrices is a central skill in data science, and in particular in relation to not only deep learning but high dimensional data sources (networks, text and images). So grab the chance to brush up.\n",
    "\n",
    "Secondly we shall practice our understanding of what actually goes on inside the neural network, here we shall practice reading network represenations of what can be called the computational graph that a neural network is. We will practice on small and simple networks, but understanding these basic mechanics is crucial before we can move towards more abstract representations of larger and more complex neural network architectures (Convulational neural networks, Recurrent Neural Networks, Transformers,  AutoEncoder), that we will look more into in the next lecture. \n",
    "\n",
    "Thirdly we will start practicing the mechanics of building and fitting a neural network model using the ```keras``` deep learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNxDTcKugZBK"
   },
   "source": [
    "# 1 Linear Algebra\n",
    "The purpose of these exercises is to practice linear algebra - vector and matrix (and tensor) operations - and to get acquinted with the ```\n",
    "numpy``` library for doing vector and matrix operations.\n",
    "\n",
    "Vector and matrix operations are central to among other things deep learning. Operations such as Dot product, and Matrix Inversion can be thought of as ```for``` loops describing simple operations of addition and multiplication, in a specific order related to the rows and columns.\n",
    "\n",
    "Here we practice the mechanics both by implementing them as ```for```-loops and using the ```numpy``` python package.\n",
    "\n",
    "## Intro (SKIP if superflous)\n",
    "### Vectors and Matrices\n",
    "In ```\n",
    "numpy```, we use the ```\n",
    "numpy.array``` to represent both vectors and matrices. Arrays are like lists, they are just indexed containers for any object, however they have a fixed size. \n",
    "If an array contain only numbers (scalars), it is a vector.\n",
    "```python\n",
    "a = np.array([4,3,2,1])# a is a vector\n",
    "```\n",
    "Vectors values can be accesed using standard list notation:\n",
    "```python\n",
    "a[0]\n",
    "> 4\n",
    "a[-1]\n",
    "> 1\n",
    "```\n",
    "\n",
    "### Matrices\n",
    "If an array contains a list of vectors of equal size (same length) it is a matrix. In this regard we can think of matrices simply as lists of lists.\n",
    "```python\n",
    "M = np.array([[1,2,3,4],\n",
    "              [3,4,5,6],\n",
    "              [2,3,1,0]])# M is a Matrix\n",
    "```\n",
    "Because the vectors have the same length, the matrix is rectangular, and therefore values can be accessed using row and column indices. You can use standard python list syntax ```[]```, as well as a more convenient numpy style.\n",
    "\n",
    "```python\n",
    "## Python list of lists syntax\n",
    "M[0][0]# index 0 in first list, index 0 in list of list.\n",
    "> 1\n",
    "## numpy array syntax \n",
    "a[0,3] # row index 0 and column index 3\n",
    "> 4\n",
    "```\n",
    "You can slice, again using standard list syntax, as well as more convenient numpy syntax:\n",
    "\n",
    "```python\n",
    "## Python list of lists syntax\n",
    "M[1:3][0:2]# two second lists, two first elements of lists.\n",
    ">  np.array([[3,4],\n",
    "              [2,3]])\n",
    "## numpy array syntax \n",
    "a[1:3,0:2] # row index 1 and 2 and column index 0 and 1\n",
    ">  np.array([[3,4],\n",
    "              [2,3]])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoHds-RygZBL"
   },
   "source": [
    "### Vector dot product\n",
    "The *vector-to-vector* dotproduct is essentially a weighed sum between two vectors (vectors are just lists of scalar numbers). Where values are paired by index. Weighing means that paired values (i.e. same index) are multiplied. \n",
    "\n",
    "The dot product between vectors a and b: $a\\cdot b = \\sum_{i}^{n}a_i b_i = a_1b_1+a_2b_2 ... a_nb_n$\n",
    "\n",
    "The dot product between the vector ```a = [1,2,3,4]``` and ```b = [4,3,2,1]``` is 20. \n",
    "\n",
    "\n",
    "**Exercise 1.1**: Write a function that computes the dot product (i.e. weighed sum) between two lists. Arguments to the function should be called x (inputs) and w (weights). \n",
    "- Write a function ```w_sum``` that multiplies index paired values and ```return``` the ```sum```. \n",
    "  1. Define a **for** loop that iterates through the indices of the two lists (hint: use the ```range()``` function, multiply the two values, and increment the resulting value to a variable *output*.\n",
    "  2. Return the *output* value.\n",
    "- Verify that it works by computing the w_sum of vector a and b (defined above).\n",
    "- Now cast vector a and b as a ```numpy``` array - ```np.array()``` - and compute the ```.dot``` product. using the built in ```.dot``` method of the array object.\n",
    "- Generate two random vectors of size 1000 using the ```np.random.random``` function, and compare the effiency (i.e. the time it takes to compute) of the  ```w_sum``` and the numpy ```.dot``` product. \n",
    "  - This can either be done using the ```time``` module, or the ```%timeit ``` notebook functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7026,
     "status": "ok",
     "timestamp": 1620638067164,
     "user": {
      "displayName": "Terne Thorn Jakobsen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giyf9v1H1h4iteKo0T2fl5hHuAr9jxrMjcMwTDW=s64",
      "userId": "17600962664087183096"
     },
     "user_tz": -120
    },
    "id": "XFjIyoNVgZBL",
    "outputId": "2e6fa1da-ae49-46ec-d8d7-7c66c7bdd173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_sum:  20\n",
      "numpy .dot:  20\n",
      "w_sum time: 0.054116249084472656\n",
      "NUMPY DOT: 0.0010061264038085938\n",
      "10 loops, best of 5: 48.8 ms per loop\n",
      "10000 loops, best of 5: 46.7 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "## Answer\n",
    "a = [1,2,3,4]\n",
    "b = [4,3,2,1]\n",
    "\n",
    "def w_sum(a,b):\n",
    "  output = 0\n",
    "  for i in range(len(a)):\n",
    "    output+=a[i]*b[i]\n",
    "  return output\n",
    "\n",
    "print('w_sum: ',w_sum(a,b))\n",
    "## Numpy version\n",
    "a,b = np.array(a),np.array(b)\n",
    "print('numpy .dot: ',a.dot(b))\n",
    "## Test efficiency\n",
    "v,u = np.random.random(100000),np.random.random(100000)\n",
    "import time\n",
    "import time\n",
    "t = time.time()\n",
    "w_sum(v,u)\n",
    "print('w_sum time:',time.time()-t)\n",
    "t = time.time()\n",
    "v.dot(u)\n",
    "print('NUMPY DOT:',time.time()-t)\n",
    "%timeit w_sum(v,u)\n",
    "%timeit v.dot(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbGWA3hHgZBN"
   },
   "source": [
    "## Matrix operations: Transposing and slicing.\n",
    "Here we practice generating matrices, transposing and slicing them. \n",
    "\n",
    "**Exercise 1.2**\n",
    "Construct a 10 times 10 matrix like so:\n",
    "```\n",
    "M = np.ones((10,10))\n",
    "M = M * np.random.choice(np.arange(10),(10,10))\n",
    "```\n",
    "- Practice the builtin functions of the numpy array: ```.sum(),.mean(),.max()```\n",
    "  1. Take the sum of the matrix.\n",
    "  2. Take the sum along ```axis=1```, use the ```axis``` argument of the sum function. Is this the row or the column sum?\n",
    "  3. Take the mean of each row using the ```.mean()``` builtin function (use the right axis). \n",
    "  4. Define the transposed matrix ```M_t``` using the ```.T``` builtin function. Explain what happened. \n",
    "\n",
    "- Slice the matrix into different shapes.\n",
    "  - Define a smaller matrix ```S``` as submatrix of M: Using the numpy slicing syntax (```M[i:j,h:k]```) extract all values of rows 1-4 of columns 3 through 8.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1620634255150,
     "user": {
      "displayName": "Terne Thorn Jakobsen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giyf9v1H1h4iteKo0T2fl5hHuAr9jxrMjcMwTDW=s64",
      "userId": "17600962664087183096"
     },
     "user_tz": -120
    },
    "id": "FBGaMkV4gZBO",
    "outputId": "99fb4690-d0d2-4ac6-bc0c-4a2fb80518bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8. 0. 3. 5. 7. 8. 7. 1. 5. 0.]\n",
      " [6. 9. 8. 5. 0. 4. 6. 7. 6. 4.]\n",
      " [9. 7. 4. 1. 1. 4. 0. 7. 4. 4.]\n",
      " [7. 6. 7. 5. 8. 4. 8. 6. 8. 1.]\n",
      " [7. 5. 6. 1. 1. 0. 3. 6. 4. 2.]\n",
      " [9. 3. 7. 9. 8. 9. 4. 9. 5. 3.]\n",
      " [4. 4. 3. 3. 5. 0. 2. 4. 3. 6.]\n",
      " [4. 6. 7. 4. 8. 3. 2. 3. 0. 7.]\n",
      " [2. 1. 1. 8. 7. 2. 9. 9. 5. 8.]\n",
      " [1. 6. 1. 3. 5. 6. 3. 5. 5. 4.]]\n",
      "Sum of all values 470.0\n",
      "Sum of all columns (axis 1): [44. 55. 41. 60. 35. 66. 34. 44. 52. 39.]\n",
      "Mean of all rows [5.7 4.7 4.7 4.4 5.  4.  4.4 5.7 4.5 3.9]\n",
      "[[8. 6. 9. 7. 7. 9. 4. 4. 2. 1.]\n",
      " [0. 9. 7. 6. 5. 3. 4. 6. 1. 6.]\n",
      " [3. 8. 4. 7. 6. 7. 3. 7. 1. 1.]\n",
      " [5. 5. 1. 5. 1. 9. 3. 4. 8. 3.]\n",
      " [7. 0. 1. 8. 1. 8. 5. 8. 7. 5.]\n",
      " [8. 4. 4. 4. 0. 9. 0. 3. 2. 6.]\n",
      " [7. 6. 0. 8. 3. 4. 2. 2. 9. 3.]\n",
      " [1. 7. 7. 6. 6. 9. 4. 3. 9. 5.]\n",
      " [5. 6. 4. 8. 4. 5. 3. 0. 5. 5.]\n",
      " [0. 4. 4. 1. 2. 3. 6. 7. 8. 4.]]\n",
      "[[5. 0. 4. 6. 7.]\n",
      " [1. 1. 4. 0. 7.]\n",
      " [5. 8. 4. 8. 6.]]\n"
     ]
    }
   ],
   "source": [
    "## Answer.\n",
    "M = np.ones((10,10))\n",
    "M = M * np.random.choice(np.arange(10),(10,10))\n",
    "print(M)\n",
    "# SUM of all values\n",
    "print('Sum of all values',M.sum())\n",
    "# Sum of all rows.\n",
    "print('Sum of all columns (axis 1):',M.sum(axis=1))\n",
    "# Column mean\n",
    "print('Mean of all rows',M.mean(axis=0))\n",
    "# Take the transpose\n",
    "M_t =  M.T\n",
    "print(M_t)\n",
    "# Submatrix\n",
    "S = M[1:4,3:8]\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g5gEMLZgZBO"
   },
   "source": [
    "## Matrix-Matrix dot product\n",
    "Now it becomes a little more tricky, in the sense that we are still doing simple operations, but there are more indices to keep track of. \n",
    "\n",
    "Vectors only have row indices, that we use to pair values. But how should we pair a matrix with both row and column indices? \n",
    "By mathematical convention we pair **all** row vectors with **all** column vectors. Because we match rows to columns, if we want to calculate the .dot product between matrix A and B, we need to make sure that the number of rows in matrix A matches the number of columns in matrix B.\n",
    "```python\n",
    "A = [\n",
    "  [4. 1. 0. 1.] # row 0\n",
    "  [0. 3. 3. 1.] # row 1\n",
    "]\n",
    "B = [\n",
    "  [0. 1.] # row 0\n",
    " [3. 1.]  # row 1\n",
    " [0. 4.]  # row 3\n",
    " [2. 1.]  # row 4\n",
    "]\n",
    "\n",
    "``` \n",
    "In this case A is a 2x4 matrix and B is a 4x2. \n",
    "So the dot product between the matrices can be thought of as two nested for loops, where we iterate through all row vectors of matrix A matching them with all columns vectors of matrix B.\n",
    "With every row and column pair, we compute a dot product. \n",
    "\n",
    "This furthermore means that the order matters - i.e. that A dot B is not the same as B dot A. \n",
    "\n",
    "**Exercise 1.3: Matrix to matrix dot products**\n",
    "\n",
    "Again we want to get a practical feel of the mechanics by implementing the operation ourselves using basic python. We already defined the ```w_sum``` for doing vector dot products. Now we want to write a function that matches row vectors from one matrix to column vectors in another and computes the ```w_sum```.\n",
    "We will work with the matrices A, B defined here:\n",
    "```python\n",
    "A = M[0:2,0:4]\n",
    "B = M[0:4,2:4]\n",
    "```\n",
    "\n",
    "- Write function ```multiple_wsum(A,B)``` with arguments A and B as two numpy arrays. In the function, follow these steps:\n",
    "  1. Define a list called `output`.\n",
    "  2. Run through all row vectors/indices *i* in A. \n",
    "    - Define temporary list for storing the dot products.\n",
    "    - Extract the ith row vector of `A`\n",
    "    - Run through all column vectors/indices *j* in `B`\n",
    "      - Compute `w_sum` (or `.dot`) between the i'th row in A and j'th column in B\n",
    "      - Append to tempoary list\n",
    "    - Append temporary list to `output` list.\n",
    "  3. Return `output`\n",
    "\n",
    "- Validate that your function outputs the same as the numpy equivalent: ```A.dot(B)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1620641058042,
     "user": {
      "displayName": "Terne Thorn Jakobsen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giyf9v1H1h4iteKo0T2fl5hHuAr9jxrMjcMwTDW=s64",
      "userId": "17600962664087183096"
     },
     "user_tz": -120
    },
    "id": "0JSLUA7agZBP",
    "outputId": "e9f500ef-3d36-4907-df4c-87f79fc5514e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8. 0. 3. 5.]\n",
      " [6. 9. 8. 5.]]\n",
      "[[3. 5.]\n",
      " [8. 5.]\n",
      " [4. 1.]\n",
      " [7. 5.]]\n",
      "2\n",
      "(4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[71.0, 68.0], [157.0, 108.0]], array([[ 71.,  68.],\n",
       "        [157., 108.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = M[0:2,0:4]\n",
    "B = M[0:4,2:4]\n",
    "print(A), print(B)\n",
    "print(A.shape[0])\n",
    "print(B.shape)\n",
    "\n",
    "# Answer\n",
    "def multiple_wsum(A,B):\n",
    "  output = []\n",
    "\n",
    "  for i in range(A.shape[0]):\n",
    "    a = A[i]\n",
    "    temp = []\n",
    "    for j in range(B.shape[1]):\n",
    "      b = B[:,j]\n",
    "      temp.append(w_sum(a,b))\n",
    "    output.append(temp)\n",
    "  return output\n",
    "multiple_wsum(A,B),A.dot(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9W_BJpZgZBP"
   },
   "source": [
    "# 2. Forward- and backpropagation\n",
    "\n",
    "**Exercise 2.1**: Pen and Paper Neural Networks: Forward propagation.\n",
    "\n",
    "For this exercise I want you to do the calculations by hand. \n",
    "Given the following inputs: \n",
    "```\n",
    "inputs = [\n",
    "[1, 4, -2],\n",
    "[1, 4, -4],\n",
    "[1, 3, 1]]\n",
    "```\n",
    "And the following neural network:\n",
    "![](https://raw.githubusercontent.com/snorre87/instructional_images/master/deeplearning_course/penpaper_exercise_forward.png)\n",
    "\n",
    "Calculate the forward pass by hand (or if you are up for a challenge implement it as a python function.)\n",
    "\n",
    "Remember each node (discounting the input layer) has two parts. First it computes a weighted sum of inputs (the result of nodes from previous layer) using the weights described by the edges. ```w_sum = weighs[0] * input[0] + weighs[1] * input[1] + weighs[2] * input[2]```. \n",
    "After doing the weighed sum, a \"kernel\" / activation function is applied to the sum. In this case either a linear kernel (i.e. no kernel) or the ReLU function: \n",
    "```python \n",
    "def linear(val): \n",
    "  return val ## I do nothing :)\n",
    "  \n",
    "def relu(val): \n",
    "  return max(0,val)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1620639860203,
     "user": {
      "displayName": "Terne Thorn Jakobsen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giyf9v1H1h4iteKo0T2fl5hHuAr9jxrMjcMwTDW=s64",
      "userId": "17600962664087183096"
     },
     "user_tz": -120
    },
    "id": "H8U9okMWgZBQ",
    "outputId": "cc63e68d-25a8-45ae-e4e5-3af6588e9e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.999999999999996]\n",
      "[32.699999999999996]\n",
      "[-2.1700000000000017]\n"
     ]
    }
   ],
   "source": [
    "# Answer DONE using Python instead of pen and paper.\n",
    "\n",
    "inputs = [\n",
    "[1, 4, -2],\n",
    "[1, 4, -4],\n",
    "[1, 3, 1]]\n",
    "\n",
    "WS = [[[-4.1, 2.1, -2.6, -2.1], [-4.9, -0.4, 0.3, 1.4], [4.7, 1.8, -2.3, 2.6]],\n",
    " [[-2.4], [-3.8], [-2.7], [4.5], [1.6]]] # Weights\n",
    "\n",
    "def relu(x):\n",
    "  return max(0.0,x)\n",
    "\n",
    "def linear(x):\n",
    "\treturn x\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn 1.0 / (1.0 + np.exp(-x))\n",
    " \n",
    "kernels = {'sigmoid':sigmoid,'linear':linear,'relu':relu}\n",
    "\n",
    "def neural_net(inp,WS,activation):\n",
    "  # keep the node states for visualization.\n",
    "  layer = 0 \n",
    "  node2state = {(layer,num):i for num,i in enumerate(inp)}\n",
    "  # Iterate through each layers' weights\n",
    "  for layer,ws in enumerate(WS):\n",
    "    ### Add bias term to the input of next layer.\n",
    "    # Check if output layer.\n",
    "    if layer != (len(WS)-1): \n",
    "      out = [1] # add bias term.\n",
    "    else: # If output layer we do not add bias term to the next layer - as there are no next layer :)\n",
    "      out = []\n",
    "    # Match nodes to weights. \n",
    "    ws = np.array(ws).T # Transpose weights to match nodes to weights\n",
    "    for w in ws: # Iterate through all nodes in the layer.\n",
    "      # compute weighed sum between input and weights.\n",
    "      w_sum = 0\n",
    "      for i,wi in zip(inp,w):\n",
    "        w_sum+=(i*wi)\n",
    "      # apply activation function.\n",
    "      a = activation[layer]\n",
    "      w_sum = kernels[a](w_sum)\n",
    "      out.append(w_sum)\n",
    "    inp = out # Define input to the next layer as the output of previous.\n",
    "    # Log node states.\n",
    "    for num,val in enumerate(out):\n",
    "      node2state[(layer+1,num)] = val\n",
    "  return out,node2state\n",
    "\n",
    "for inp in inputs:\n",
    "  out,node2state = neural_net(inp,WS,['relu','linear'])\n",
    "  print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL2Z21LYgZBS"
   },
   "source": [
    "**Exercise 2.2**: \"Backpropagation\" intution\n",
    "\n",
    "Instead of calculating the actual gradient (which keras will do for us automatically), we test our intuition. \n",
    "\n",
    "\n",
    "Looking at the figure below you can see that the result of forward propagating the inputs ```[1,3,1]``` were -2.17. Imagine that the network was suppose to output 5 (i.e. the training label y = 5). Now go through the weights (edges) in the network from the last layer to the output layer and write if you would increase or decrease them. \n",
    "\n",
    "- Fill in the python dictionary of the network nodes with -1 if decrease and 1 if increase and 0 if keeping that weight as is.\n",
    "\n",
    "![](https://raw.githubusercontent.com/snorre87/instructional_images/master/deeplearning_course/penpaper_solution.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aKZJ1SfSgZBS"
   },
   "outputs": [],
   "source": [
    "## Fill in -1 if decrease and 1 if increase\n",
    "gradient = {\n",
    " ('bias_1_0', 'linear_2_0'):0,#  {'weight': -2.4}\n",
    " ('relu_1_1', 'linear_2_0'):0, #  {'weight': -3.8}\n",
    " ('relu_1_2', 'linear_2_0'):0, #  {'weight': -2.7}\n",
    " ('relu_1_3', 'linear_2_0'):0, # {'weight': 4.5}\n",
    " ('relu_1_4', 'linear_2_0'):0} #  {'weight': 1.6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EmBeKMUCgZBT"
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "gradient = {\n",
    " ('bias_1_0', 'linear_2_0'):1,#  {'weight': -2.4}\n",
    " ('relu_1_1', 'linear_2_0'):0, #  {'weight': -3.8}  \n",
    " ('relu_1_2', 'linear_2_0'):1, #  {'weight': -2.7} \n",
    " ('relu_1_3', 'linear_2_0'):0, # {'weight': 4.5}  \n",
    " ('relu_1_4', 'linear_2_0'):1} #  {'weight': 1.6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbUKtVcQvcvL"
   },
   "source": [
    "# 3 Keras exercises\n",
    "Now that you have a good understanding of what happens in a neural networks (that it is basically vector and matrix multiplications to get weighted sums) it is time to get your hands on a tool that makes life a lot more easy for you. The goal of these exercises is to make you familiar with the deep learning framework ```Keras```, specifically getting familiar with the syntax and how to write a simple model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kURKb_ihZPjF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y08TnokZ8OA",
    "outputId": "0d7b26b2-d4b8-40d2-cbcb-c441a830c9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Note that using different versions of tensorflow as backend may result in some \n",
    "# errors do to differences in method and attribute names as well as syntax. \n",
    "# But these errors are easy to fix when you are aware of it.\n",
    "print(tf.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYzl-rEWEW0h"
   },
   "source": [
    "### Data\n",
    "\n",
    "We will be working with the MNIST dataset in these exercises. (Read about it here https://keras.io/api/datasets/mnist/ or https://en.wikipedia.org/wiki/MNIST_database). A small disclamer: it is very easy to reach high accuracies (high performance) with this type of \"introductional\" data. With data \"in the wild\", which you may collect yourselves, you can rarely expect the same performance!\n",
    "\n",
    "The data is imported and prepared for you in the following three code cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "127tcWyL2fHl",
    "outputId": "23ec9c9b-2cf7-4c71-f207-cf4d187e1e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9ccd341690>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# view/plot the first element in the training data:\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DYEYpcKUZGNF"
   },
   "outputs": [],
   "source": [
    "# print the first element in the training data:\n",
    "print(x_train[0].shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNdd9KrkYeND"
   },
   "source": [
    "The MNIST dataset consists of handwritten digits from 0-9, meaning there are 10 classes. However, we want to use it for some binary classification in these exercises, so we'll make a dataset consisting only of the digits 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNaD57Qt5dnT",
    "outputId": "b866f7fe-6ac3-4207-df87-99ef15f8f402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now we only have the labels 0 and 8 in our data:  [0 8 0 8 0 0 8 8 0 8]\n",
      "now we only have the labels 0 and 8 in our data but we change 8 to label 1 for binary prediction:  [0 1 0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "x_train_new, y_train_new = x_train[(y_train==0) | (y_train==8)], y_train[(y_train==0) | (y_train==8)]\n",
    "x_test_new, y_test_new = x_test[(y_test==0) | (y_test==8)], y_test[(y_test==0) | (y_test==8)]\n",
    "print(\"now we only have the labels 0 and 8 in our data: \",y_train_new[:10]) \n",
    "y_train_new = np.array([1 if i==8 else i for i in y_train_new])\n",
    "y_test_new = np.array([1 if i==8 else i for i in y_test_new])\n",
    "print(\"now we only have the labels 0 and 8 in our data but we change 8 to label 1 for binary prediction: \",y_train_new[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9KldMNKY7za"
   },
   "source": [
    "We also want to flatten the images. Flattening means that we are making a multi-dimensional matrix one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8B-6GmMRY7hi",
    "outputId": "5813a511-500c-4448-e786-af94fa48ac77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After taking a subset of the data and flattening:\n",
      "x_train shape: (11774, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train_new = x_train_new.reshape((-1, 784)) \n",
    "x_test_new = x_test_new.reshape((-1, 784))\n",
    "print(\"After taking a subset of the data and flattening:\")\n",
    "print(\"x_train shape:\", x_train_new.shape) # i.e. 12665 vectors (training samples) of length 784!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uz_k4tmdwN8B"
   },
   "source": [
    "Since we want some validation data as well, we split our training data in two parts and check the new sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIVLw-2ieHFO",
    "outputId": "f7b73006-7d90-48bc-99cc-db8183039eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5887, 784)\n",
      "x_val shape: (5887, 784)\n",
      "x_test shape: (1954, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train_new, X_val, y_train_new, y_val = train_test_split(x_train_new, y_train_new, test_size=0.5)\n",
    "print(\"x_train shape:\", x_train_new.shape)\n",
    "print(\"x_val shape:\", X_val.shape)\n",
    "print(\"x_test shape:\", x_test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZTYuQOHZeT_"
   },
   "source": [
    "## 3.1 The sequential model\n",
    "\n",
    "There are two ways to write models in Keras: by using the *sequential* or the *functional* API. The sequential model may be more intiuitive or easy to read, but the functional has (unsurprisingly) more functionality/flexibility. We'll start be introducing you to the sequential model, but for the remainder of the course, you will be asked to use the functional API.\n",
    "\n",
    "**3.1.1**: Read the first four sections in the documentation for the sequential model https://keras.io/guides/sequential_model/. \n",
    "\n",
    "**3.1.2**: In the below cell we have defined a simple fully-connected (i.e. *dense*) neural network. Do you recoginize the model? Knowing what the `Dense` layer is, and what the `Sigmoid` activation does (from the documentation), can you then guess what this model may also be called?\n",
    "\n",
    "**Answer**: It is simply logistic regression.\n",
    "\n",
    "**3.1.3**: Why is input_shape specified as (784,)?\n",
    "\n",
    "**Answer**: (784,) is the shape of one **flattened** image in our MNIST dataset. I.e. each input is a vector of 784 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0ffR47AoZdkQ"
   },
   "outputs": [],
   "source": [
    "# a simple fully-connected neural network\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(1, activation=\"sigmoid\", input_shape=(784,))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M68BNLCI__v7"
   },
   "source": [
    "**3.1.4**: To train a model in Keras, you need at least two steps: firstly, you `compile` the model and secondly, you `fit` the compiled model. Below we have written these two steps for you for reference. Please read the comments.\n",
    "\n",
    "\n",
    "When a model has been `fit`, you can evaluate it on test data. You can use the `evaluate` function (writing model.evaluate(X,y)) to evaluate the model directly, given your test data, and/or you can use the `predict` function (model.predict(X)) to retrieve the predictions and envestigate these. Use both the evaluate and predict function below with x_test_new and y_test_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTpXr4VE91TB",
    "outputId": "abf8da1e-5046-44a7-b26d-96b584679170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "184/184 [==============================] - 0s 931us/step - loss: 0.0748\n",
      "Epoch 2/3\n",
      "184/184 [==============================] - 0s 994us/step - loss: 0.0718\n",
      "Epoch 3/3\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f48cfd06c10>"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steps needed to train and test a model\n",
    "\n",
    "# Before fitting/training your model in Keras, you need to firstly \"compile\" it. Here you specify the optimizer, loss function and evalution metric(s) you want to use.\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy') \n",
    "\n",
    "# Then you are ready to train/fit your model. \n",
    "model.fit(x_train_new, y_train_new, epochs=3)\n",
    "# You can give the fitting process a name to be able to access information such as the loss at each epoch later on. In this case, you would instead write:\n",
    "# history = model.fit(x_train_new, y_train_new, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Le9S8ezsBrdk"
   },
   "outputs": [],
   "source": [
    "loss = # your code here\n",
    "print(loss)\n",
    "\n",
    "predictions = #your code\n",
    "print(\"The first ten predictions:\",predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj9_4v9tA4t0",
    "outputId": "3c48306b-4588-48d0-e9cb-eb59d9f10d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 925us/step - loss: 0.0535\n",
      "0.053522929549217224\n",
      "The first ten predictions: [[1.23181939e-03]\n",
      " [3.04135978e-02]\n",
      " [1.50713027e-02]\n",
      " [5.41865826e-04]\n",
      " [9.27537680e-03]\n",
      " [1.65422857e-01]\n",
      " [8.89950275e-01]\n",
      " [1.09550655e-02]\n",
      " [3.00538540e-03]\n",
      " [9.81952667e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "loss = model.evaluate(x_test_new,y_test_new)\n",
    "print(loss)\n",
    "\n",
    "predictions = model.predict(x_test_new) \n",
    "print(\"The first ten predictions:\",predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am8wAONsHLh9"
   },
   "source": [
    "**3.1.5**: Now it is your turn to define a model and train it from scratch. Do the following:\n",
    "- Copy the model above and paste it to the cell below. Change the name of the model to model2.\n",
    "- Add one Dense layer before the current layer. This makes it a **multi-layer perceptron**! Use any number of units/neurons you think is appropriate. But make sure to specify your input shape(s) correctly (or, at least not wrongly)! If you are having trouble with dimensions, this might help: https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc \n",
    "- Compile, fit and evaluate your new model2, but this time add `metrics=\"binary_accuracy\"` to the compile function. When you evaluate your model now, you should get two outputs; a loss and an accuracy. Also, give your fitting process a name such as \"history\" and change the number of epochs to 10 or more.\n",
    "- Plot the loss over each epochs by accesing the \"history\" object you defined when fitting. Hint: `history.history` returns a dictionary. Get the loss values from the dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "cDGs_I0XrVvc"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "nK5wdn2ouqQe",
    "outputId": "fce7404d-3950-4ba8-98d2-71aaa2e0ae26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 1s 2ms/step - loss: 0.2961 - binary_accuracy: 0.8934\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0775 - binary_accuracy: 0.9822\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0545 - binary_accuracy: 0.9854\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0472 - binary_accuracy: 0.9876\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0393 - binary_accuracy: 0.9871\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0384 - binary_accuracy: 0.9870\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0376 - binary_accuracy: 0.9884\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0303 - binary_accuracy: 0.9918\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0302 - binary_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 0s 1ms/step - loss: 0.0310 - binary_accuracy: 0.9882\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0311 - binary_accuracy: 0.9923\n",
      "0.031067760661244392 0.9923234581947327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f48bf6b64d0>]"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#solution:\n",
    "model2 = keras.Sequential(\n",
    "    [   \n",
    "        layers.Dense(64, input_shape=(784,)),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.compile(optimizer='sgd', loss='binary_crossentropy', metrics=\"binary_accuracy\") \n",
    "history = model2.fit(x_train_new, y_train_new, epochs=10)\n",
    "loss, accuracy = model2.evaluate(x_test_new,y_test_new)\n",
    "print(loss, accuracy)\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNSIvKUJxpY2"
   },
   "source": [
    "**3.1.6**: Now, take a look at the model defined below. What is the number of trainable parameters of \"example_model\"? Explain the number of parameters (weights and biases) for each layer. Note that, so far, you have only been shown feed-forward neural networks (the Dense layer is a feed-forward layer). *Hint: you may want to look into the model.summary() method to verify your calculation.*\n",
    "\n",
    "**Answer**: With only one input feature and 32 units in the first layer, there are 1x32 weights + 32 bias terms (one for each unit) of the first layer, hence 64 trainable parameters of the first layer. The input to the second layer is of size 32 (equal to the number of units of the first layer), so with only 1 unit in the second layer we have 32x1 weights + one bias term in the second layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3cPxjDdLxs5K"
   },
   "outputs": [],
   "source": [
    "example_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(32, input_shape=(1,), activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBLdZtot12iw",
    "outputId": "c2e9313c-bae7-4186-a7f7-5b6d4da8b58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "example_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrrt46uVqjXG"
   },
   "source": [
    "**OPTIONAL 3.1.7**: In the documentation you can see that you can also \"create a Sequential model incrementally via the `add()` method\". Re-write \"example_model\" using the `add` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "TM1nnwmJrSGQ"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "t5MHdr6mPYiq"
   },
   "outputs": [],
   "source": [
    "# solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ39k6lAZi-W"
   },
   "source": [
    "## 3.2 The functional model \n",
    "Now we will practice writting models with the functional API, which we will continue with further on. In terms of syntax, you will only see a difference when defining the models, meaning that the training and evaluation steps (`compile`, `fit` and `evalute`/`predict`) are the same no matter which API, the sequential or functional, you are using.\n",
    "\n",
    "**3.2.1**: Read the first three sections of the documentation https://keras.io/guides/functional_api/\n",
    "\n",
    "**3.2.2**: Re-write \"example_model\" using the functional API and name it model3. Change the input shape such that we can use the model with our MNIST data (784,).\n",
    "\n",
    "**3.2.3**: Compile, fit and evaluate model3, but this time add the validation data, X_val and y_val, to the `validation_data` argument in `fit`. The argument expects a tuple (see https://keras.io/api/models/model_training_apis/#fit-method).\n",
    "\n",
    "**3.2.4** Plot loss and validation accuracy (`'val_binary_accuracy'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vYBWtGBtrqWB"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "i-85roJ5xacB",
    "outputId": "897cbf5c-7cb7-4195-d0e4-9289d76caa0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 1s 3ms/step - loss: 0.4590 - binary_accuracy: 0.8290 - val_loss: 0.1396 - val_binary_accuracy: 0.9711\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 0s 3ms/step - loss: 0.1208 - binary_accuracy: 0.9730 - val_loss: 0.0809 - val_binary_accuracy: 0.9796\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0715 - binary_accuracy: 0.9840 - val_loss: 0.0623 - val_binary_accuracy: 0.9815\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0648 - binary_accuracy: 0.9821 - val_loss: 0.0530 - val_binary_accuracy: 0.9847\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0534 - binary_accuracy: 0.9851 - val_loss: 0.0473 - val_binary_accuracy: 0.9857\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0446 - binary_accuracy: 0.9861 - val_loss: 0.0437 - val_binary_accuracy: 0.9864\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0419 - binary_accuracy: 0.9895 - val_loss: 0.0422 - val_binary_accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0365 - binary_accuracy: 0.9880 - val_loss: 0.0388 - val_binary_accuracy: 0.9881\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 0s 3ms/step - loss: 0.0367 - binary_accuracy: 0.9878 - val_loss: 0.0372 - val_binary_accuracy: 0.9888\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 0s 2ms/step - loss: 0.0324 - binary_accuracy: 0.9915 - val_loss: 0.0365 - val_binary_accuracy: 0.9891\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0333 - binary_accuracy: 0.9908\n",
      "0.033269014209508896 0.9907881021499634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f48beebe310>]"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solution:\n",
    "inputs = keras.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model3 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model3.compile(optimizer='sgd', loss='binary_crossentropy', metrics=\"binary_accuracy\") \n",
    "history = model3.fit(x_train_new, y_train_new, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "loss, accuracy = model3.evaluate(x_test_new,y_test_new)\n",
    "print(loss, accuracy)\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_binary_accuracy'], label=\"accuracy\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7MPmQzVbp2dA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep_learning_1_exercises_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
