{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm #to create a progress bar\n",
    "\n",
    "#Packages for NLP\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Machine learning packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Packages for cross-validation and parameter tuning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Packages for getting model performance metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning \n",
    "\n",
    "For this exercise lab, the goal is to apply supervised machine learning to predict which tweets are from Republicans and from Democrats on Twitter. \n",
    "\n",
    "The dataset is the same as the one we used Monday and last week, containing tweets from US Members of Congress. The preprocessed version is also the same as the one used on Monday. \n",
    "\n",
    "We will be applying two supervised machine learning algorithms - random forest and lasso - and comparing which method performs best when predicting party affiliation from tweet text. \n",
    "\n",
    "\n",
    "### 1.1: Preparing the data for analysis \n",
    "\n",
    "1. Import the dataframe\n",
    "2. Similar to last week, replace NaN values with an empty string in the stemmed text. Then use `groupby` and `agg` to group the data by politician (nominate_name), and aggregate the stemmed tweet text for each politician into one long string. \n",
    "3. Create a column with a binary label to show party affiliation. 0 if Republican, 1 if Democrat. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Creating a validation set and splitting features (X) and labels (y)\n",
    "\n",
    "We'll pretend that we only know for 300 of the politicians whether they are Republicans or Democrats. For the remaining politicians, we therefore don't know the author's partisanship. Our goal then, is to use machine learning models to predict whether a user is a Republican or Democrat using the 218 labeled observations in our training set. Then we'll use a machine learning model that we have fit to the training data to predict the label for the unlabeled politicians.\n",
    "\n",
    "1. Split the dataset into two: one labeled and one unlabeled. You can use `sample` on the aggregated dataframe to get a random sample of 300 politicians for the unlabeled dataset. The labeled dataset should be the remaining 218 politicians. \n",
    "3. Create a training set by splitting the labeled data into: X (the stemmed text) and y (the newly created binary label column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labeled dataset: (218, 6) \n",
      "Shape of unlabeled dataset: (300, 6)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: RandomForest: Hyperparameter tuning\n",
    "\n",
    "Now we begin with the supervised learning. First, we will train and tune a RandomForest classifier. Find the documentation for RandomForest here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html \n",
    "\n",
    "To use text data for prediction, we need to convert the data into a document-feature matrix. We will be comparing two methods of vectorization: a vectorizer using term frequencies and a transformation of those into tf-idf frequencies. \n",
    "\n",
    "\n",
    "Create a **pipeline** containing: \n",
    "\n",
    "1. `CountVectorizer`: \n",
    "    - Like Monday, we want to include both unigrams and bigrams. `CountVectorizer` can do this for us with the parameter `ngram_range`. \n",
    "    - Use the parameters `max_df` and `min_df` to remove very frequent (those that appear in more than 99.9% of the documents) and very infrequent words (those that appear in less than 10.0% documents).\n",
    "    - CountVectorizer has a build-in tokenizer. However, if you want to use the `TweetTokenizer` we used on Monday, you can override the default tokenization with your own defined function, like so: `CountVectorizer(tokenizer=your_tokenizer.tokenize)`. <br>\n",
    "\n",
    "\n",
    "2. `TfidfTransformer()` <br>\n",
    "\n",
    "\n",
    "3. `RandomForestClassifier()`\n",
    "\n",
    "\n",
    "Create a **parameter-grid** containing: \n",
    "\n",
    "1. To easily test the use of either term frequencies or tf-idf frequencies as part of your hyperparameter-tuning, use the parameter `use_idf` in the `TfidfTransformer()` in the pipeline.\n",
    "2. Experiment with the `max_features` parameter (the number of features to consider when splitting branches). To get a model that runs (fairly) quickly, try just three: [260,300,340]. Other parameters that could also be experimented with, but only if you have time, are `n_estimators` (number of trees) and `max_depth` (size of the trees).\n",
    "\n",
    "\n",
    "Use `StratifiedKFold` with 5 folds for **cross-validation**. This creates balanced distributions across the folds. \n",
    "\n",
    "Use `GridSearchCV` to **find the best RandomForest classifier**. Save the best performing model and compute the accuracy.\n",
    "\n",
    "Investigate the results. Does the count vectorized data or the tf-idf vectorized data perform better? \n",
    "\n",
    "\n",
    "Note: `TfidfVectorizer` is the same as using `CountVectorizer` followed by `TfidfTransformer`. If you at a later point, e.g. for your exam, know that you want to use the tf-idf frequencies rather than the plain term frequencies, this is an option. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tokenizer I want to use\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "#Fill in the three pipeline steps\n",
    "pipeline = Pipeline([ \n",
    "    ('#fill in here',#and here) , \n",
    "    ('#fill in here',#and here) ,\n",
    "    ('#fill in here',#and here)\n",
    "])\n",
    "\n",
    "#Fill in the parameter values in the grid \n",
    "parameter_grid = {\n",
    "    '#fill in here': #and here,\n",
    "    '#fill in here': #and here\n",
    "}\n",
    "\n",
    "#Initializing a kfold with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "#Initializing the GridSearchCV\n",
    "search = GridSearchCV(pipeline, parameter_grid, cv=cv, n_jobs = -1, verbose=10)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Running the GridSearchCV\n",
    "\n",
    "forest_result = search.fit( ) #Input your X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the best performing model and saving it \n",
    "\n",
    "#Viewing the parameters and accuracy of the best performing model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Lasso: Supervised learning and hyperparameter tuning\n",
    "\n",
    "Repeat the above steps to find the best performing lasso regression model. \n",
    "\n",
    "To implement lasso regression, we will use scikit-learn's `LogisticRegression` with `penalty = 'l1'` (which refers to the lasso penalty), `solver = 'saga'`, and bumping up `max_iter = 1000`. \n",
    "\n",
    "The parameter to cross-validate will be `C`, the inverse of regularization strength (1/Î»). Experiment with the parameter values [0.5, 1, 5].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tokenizer I want to use\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "#Initializing the pipeline\n",
    "pipeline = Pipeline([ \n",
    "    ('#fill in here',#and here) , \n",
    "    ('#fill in here',#and here) ,\n",
    "    ('#fill in here',#and here)\n",
    "])\n",
    "\n",
    "#Setting the parameter grid \n",
    "parameter_grid = {\n",
    "    '#fill in here': #and here,\n",
    "    '#fill in here': #and here\n",
    "}\n",
    "\n",
    "#Initializing a kfold with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "lasso_search = GridSearchCV(pipeline, parameter_grid, cv=cv, n_jobs = -1, verbose=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso_result = lasso_search.fit( ) #Input your X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the best performing model and saving it \n",
    "\n",
    "#Viewing the parameters and accuracy of the best performing model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Performance evaluation\n",
    "\n",
    "Now that we have our two best performing models, let's try them on data the models have never seen. Normally we couldn't \"check\" to see how well we labeled our unlabeled data, because they are... unlabeled. But for this example we actually can do that because in all of our data we know who actually is a Democrat or Republican. Split the unlabeled dataset into X_test and y_test. \n",
    "\n",
    "Fit the best models with the labeled dataset. \n",
    "\n",
    "1. Plot a confusion-matrix for the best performing RandomForest and the best performing Lasso. Scikit-learn's `plot_confusion_matrix` can do this for you. \n",
    "2. Compute accuracy, precision, recall, and f1 for each best performing model. This can be done by computing predicted y and then using `classification_report` to get the performance scores. \n",
    "3. Compute the AUC-score (area-under-the-curve) and plot the ROC-curve for each model. Code is provided. \n",
    "\n",
    "Which method performs best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting unlabeled features into a new X and y, to use as our testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy, precision, recall, and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC-score and ROC curve\n",
    "\n",
    "For this, we are borrowing code from the documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html \n",
    "\n",
    "Just fill in the blank and attempt to understand the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a binary classification - therefore, 2 classes\n",
    "n_classes = 2\n",
    "\n",
    "#Defining dictionaries to save scores for each class\n",
    "fpr = dict() #Increasing false positive rates\n",
    "tpr = dict() #Increasing true positive rates\n",
    "roc_auc = dict() #AUC-score\n",
    "\n",
    "#The predicted probabilities based on the unseen data - fill in here:\n",
    "probs =   \n",
    "\n",
    "#For each class, the dictionaries are filled in with values\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], threshold = roc_curve(ynew, probs[:, i]) #Using scikit-learn's roc_curve function to get values\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i]) #Using scikit-learn's auc function to get the auc-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "#Setting linewidth\n",
    "lw = 2\n",
    "\n",
    "#Plotting the ROC-curve for class 1 (aka prediction of Democrat) with AUC-score as the legend\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange', lw=lw, label='ROC curve (area = %0.5f)' % roc_auc[1])\n",
    "\n",
    "#Plotting the 'no skill' line, i.e. the line predicting 0 and 1 equally\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "#Adding embellishments :) \n",
    "plt.title('ROC-curve for RandomForestClassifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "#Plotting AUC-score as legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating above steps for Lasso \n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "#Fill in the predicted probabilities on the unseen data here:\n",
    "probs = \n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], threshold = roc_curve(ynew, probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the ROC-curve and AUC-score for Lasso\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "lw = 2\n",
    "\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange', lw=lw, label='ROC curve (area = %0.5f)' % roc_auc[1])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "plt.title('ROC-curve for Lasso')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting feature important for prediction\n",
    "\n",
    "Hooray, RandomForestClassifier is the best model! \n",
    "\n",
    "To see which features were important for prediction, extract the feature importances from the best performing model using `model.named_steps['insert pipeline step'].feature_importances_`. \n",
    "\n",
    "Combining these with `model.named_steps['insert pipeline step'].get_feature_names()` from the vectorizer, find the 20 most important words for prediction. \n",
    "\n",
    "Does it qualitatively make sense to you that these are the most important words to predict Democrats vs. Republicans from tweet text? \n",
    "\n",
    "If you have time, plot the 10 largest feature importances with `sns.barplot`. \n",
    "\n",
    "Hint: This can be solved similarly to the way the largest beta values and associated words were extracted in the topic modelling exercise. \n",
    "\n",
    "Optional: The most impactful coefficients can also be extracted from Lasso, in a similar way. Just remember to extract the 20 largest coefficients in *absolute* value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting feature importances from RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Extracting coefficients from Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
